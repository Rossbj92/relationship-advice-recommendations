{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import requests\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"_o78cyJghqkD-w\",\n",
    "                     client_secret=\"TBi-N01zb-0weOaD2jEBKhYYYsQ\",\n",
    "                     user_agent=\"b\")\n",
    "\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def post_df(subreddit, id):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes a subreddit name and post id and returns a dataframe of post information.\n",
    "    \n",
    "    \"\"\"\n",
    "    submission = reddit.submission(id = id)\n",
    "    post_df = pd.DataFrame(index = [master_df.shape[0] + 1])\n",
    "\n",
    "    post_df['created'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(submission.created))\n",
    "    post_df['title'] = submission.title\n",
    "    post_df['flair'] = submission.link_flair_text\n",
    "    post_df['text'] = submission.selftext\n",
    "    post_df['edited'] = submission.edited\n",
    "    post_df['ups'] = submission.ups\n",
    "    post_df['down'] = submission.downs\n",
    "    post_df['num_comments'] = submission.num_comments\n",
    "    post_df['gilded'] = submission.gilded\n",
    "    post_df['awards'] = submission.total_awards_received\n",
    "    post_df['sub'] = subreddit\n",
    "\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code adapted from https://www.reddit.com/r/pushshift/comments/bfc2m1/capping_at_1000_posts/\n",
    "\n",
    "def scraper(subreddits, posts = 35000):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a dataframe of posts. \n",
    "    \n",
    "    For each subreddit, this function queries the pushshift api.\n",
    "    IDs are then extracted and passed into the 'post_df' function\n",
    "    using Prawn to extract the relevant post information.\n",
    "    The default number of posts per subreddit is 35000. To avoid an \n",
    "    endless loop if the API stops accepting requests, exceptions are counted\n",
    "    and the loop is broken if requests fail 1000 times.\n",
    "    \n",
    "    \"\"\"\n",
    "    master_df = pd.DataFrame()\n",
    "    for sub in subreddits:\n",
    "        last = ''\n",
    "        ids = []\n",
    "        url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={sub}&fields=id,stickied,user_removed,mod_removed'\n",
    "        exceptions = 0\n",
    "\n",
    "        while len(ids) < posts:\n",
    "            try:\n",
    "                request = requests.get('{}&before={}'.format(url,last))\n",
    "                json = request.json()\n",
    "                for s in json['data']:\n",
    "                    if len(ids) < posts:\n",
    "                        if s['stickied'] == False and 'user_removed' not in s and 'mod_removed' not in s:\n",
    "                            #Making sure post is > 1 word (some pass through the 'user_removed'/'mod_removed' filter above)\n",
    "                            if len(reddit.submission(id = s['id']).selftext.split()) > 1:\n",
    "                                ids.append(s['id'])\n",
    "                                master_df = master_df.append(post_df(sub, s['id']))\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        break\n",
    "                last = int(s['created_utc'])\n",
    "\n",
    "            except:\n",
    "                #I haven't figured out a sleep time between requests, and at some point, the API will become unhappy with this many.\n",
    "                #This counts how many requests are rejected, and at 1000 rejections, breaks the loop and prints the 'last' time and \n",
    "                #Subreddit it stopped at so that you can restart the function\n",
    "                exceptions += 1\n",
    "                if exceptions == 1000:\n",
    "                    print(sub, last)\n",
    "                    break\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# data = scraper(['anxiety', 'depression', 'adhd', 'casualconversation'])\n",
    "# data.drop_duplicates(subset = ['title', 'text'], inplace = True)\n",
    "# data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#data.to_pickle('master_raw.pkl', compression = 'bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The commented out code is used to pull all 140,000 (approximately) posts. Below, I've set up a scaled down version to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = scraper(['anxiety', 'depression', 'adhd', 'casualconversation'], posts = 2)\n",
    "data.drop_duplicates(subset = ['title', 'text'], inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('postgresql://bryanross:4725A$hby@localhost:5432/redditproj')\n",
    "# data.to_sql('raw_data', engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
